## paper survey

### base model

- **[VDM] Video Diffusion Models**

    (2022.04.07) <code>NIPS 2022</code>  Google

    [\[page\]](https://video-diffusion.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2204.03458) 
    [\[code\]](https://github.com/lucidrains/video-diffusion-pytorch)
    
- **MCVD：Masked conditional video diffusion for prediction, generation, and interpolation** 

    (2022.05.19) <code>NIPS 2022</code>

    [\[page\]](https://mask-cond-video-diffusion.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2205.09853) 
    [\[code\]](https://github.com/voletiv/mcvd-pytorch)

- **[FDM] Flexible Diffusion Modeling of Long Videos**
  
    (2022.05.23) <code>arXiv</code>  

    [\[page\]](https://www.cs.ubc.ca/~wsgh/fdm/) 
    [\[paper\]](https://arxiv.org/abs/2205.11495) 
    [\[code\]](https://github.com/plai-group/flexible-video-diffusion-modeling)

- **CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers Paper**

    (2022.05.29) <code>ICLR 2023</code>  THU

    [\[page\]](https://models.aminer.cn/cogvideo/) 
    [\[paper\]](https://arxiv.org/abs/2205.15868) 
    [\[code\]](https://github.com/THUDM/CogVideo)
    
- **Make-A-Video：Text-to-Video Generation without Text-Video Data**
  
    (2022.09.29) <code>ICLR 2023</code> Meta

    [\[page\]](https://makeavideo.studio/) 
    [\[paper\]](https://arxiv.org/abs/2209.14792) 
    [\[code\]](https://github.com/lucidrains/make-a-video-pytorch)
    
- **IMAGEN VIDEO: HIGH DEFINITION VIDEO GENERATION WITH DIFFUSION MODELS**
  
    (2022.10.05) <code>arXiv</code> Google

    [\[page\]](https://imagen.research.google/video/) 
    [\[paper\]](https://arxiv.org/abs/2210.02303) 
    [code]
  
- **[PVDM] Video Probabilistic Diffusion Models in Projected Latent Space**
  
    (2023.02.15) <code>CVPR 2023</code> Google

    [\[page\]](https://sihyun.me/PVDM/) 
    [\[paper\]](https://arxiv.org/abs/2302.07685) 
    [\[code\]](https://github.com/sihyun-yu/PVDM)
    
- **MagicVideo: Efficient Video Generation With Latent Diffusion Models**

    (2022.11.20) <code>arXiv</code> ByteDance

    [\[page\]](https://magicvideo.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2211.11018) 
    [code]
    
- **LVDM：Latent Video Diffusion Models for High-Fidelity Long Video Generation**

    (2022.11.23) <code>arXiv</code> HKUST Tencent

    [\[page\]](https://yingqinghe.github.io/LVDM/) 
    [\[paper\]](https://arxiv.org/abs/2211.13221) 
    [\[code\]](https://github.com/YingqingHe/LVDM)
    
- **VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation**

    (2023.3.15) <code>CVPR 2023</code> Alibaba UCAS

    [\[page\]](https://www.modelscope.cn/models/damo/text-to-video-synthesis/summary) 
    [\[paper\]](https://arxiv.org/abs/2303.08320) 
    [\[code\]](https://www.modelscope.cn/models/damo/text-to-video-synthesis/files)
    
### video editing

- **Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation**

    (2022.12.22) <code>arXiv</code> Tencent(PCG) NUS

    [\[page\]](https://tuneavideo.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2212.11565) 
    [\[code\]](https://github.com/showlab/Tune-A-Video)

- **Shape-aware Text-driven Layered Video Editing**

    (2023.1.30) <code>CVPR 2023</code> University of Maryland

    [\[page\]](https://text-video-edit.github.io) 
    [\[paper\]](https://openaccess.thecvf.com/content/CVPR2023/html/Lee_Shape-Aware_Text-Driven_Layered_Video_Editing_CVPR_2023_paper.html) 
    [code]
    
- **Gen-1: Structure and Content-Guided Video Synthesis with Diffusion Models**

    (2023.2.6) <code>arXiv</code> Runway

    [\[page\]](https://research.runwayml.com/gen1) 
    [\[paper\]](https://arxiv.org/abs/2302.03011) 
    [code]
    
- **Video-P2P: Video Editing with Cross-attention Control**

    (2023.3.8) <code>arXiv</code> Adobe CUHK
    
    [\[page\]](https://video-p2p.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2303.04761) 
    [\[code\]](https://github.com/ShaoTengLiu/Video-P2P)
    
- **FateZero: Fusing Attentions for Zero-shot Text-based Video Editing**

    (2023.3.16) <code>ICCV 2023</code> Tencent(AI Lab) HKUST

    [\[page\]](https://fate-zero-edit.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2303.09535) 
    [\[code\]](https://github.com/ChenyangQiQi/FateZero)
    
- **Pix2video: Video editing using image diffusion**

    (2023.3.22) <code>ICCV 2023</code> Adobe

    [\[page\]](https://duyguceylan.github.io/pix2video.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2303.12688) 
    [\[code\]](https://github.com/G-U-N/Pix2Video.pytorch)
    
- **Text2video-zero: Text-toimage diffusion models are zero-shot video generators.**

    (2023.3.23) <code>arXiv</code> Picsart

    [\[page\]](https://text2video-zero.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2303.13439) 
    [\[code\]](https://github.com/Picsart-AI-Research/Text2Video-Zero)
    
- **vid2vid-zero: Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models**

    (2023.3.30) <code>arXiv</code> BAAI ZJU

    [\[page\]](https://github.com/baaivision/vid2vid-zero#examples) 
    [\[paper\]](https://arxiv.org/abs/2303.17599) 
    [\[code\]](https://github.com/baaivision/vid2vid-zero)
    
- **ControlVideo: Training-free Controllable Text-to-Video Generation**

    (2023.5.22) <code>arXiv</code> Huawei HIT

    [\[page\]](https://github.com/YBYBZhang/ControlVideo#visualizations) 
    [\[paper\]](https://arxiv.org/abs/2305.13077) 
    [\[code\]](https://github.com/YBYBZhang/ControlVideo)
    
- **ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing**

    (2023.5.26) <code>arXiv</code> Tsinghua 

    [\[page\]](https://ml.cs.tsinghua.edu.cn/controlvideo/) 
    [\[paper\]](https://arxiv.org/abs/2305.17098) 
    [\[code\]](https://github.com/thu-ml/controlvideo)
    
- **Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising**
  
    (2023.5.29) <code>arXiv</code> Shanghai AI Lab CUHK(mmLab)

    [\[page\]](https://g-u-n.github.io/projects/gen-long-video/index.html) 
    [\[paper\]](https://arxiv.org/abs/2305.18264) 
    [\[code\]](https://github.com/G-U-N/Gen-L-Video)
    
- **Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation**

    (2023.6.13) <code>arXiv</code> NTU

    [\[page\]](https://anonymous-31415926.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2306.07954) 
    [code] 

- **VidEdit: Zero-shot and Spatially Aware Text-driven Video Editing**

    (2023.6.14) <code>arXiv</code> Paris, France

    [\[page\]](https://videdit.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2306.08707) 
    [code]

- **TokenFlow: Consistent Diffusion Features for Consistent Video Editing**

    (2023.7.19) <code>arXiv</code>  Weizmann Institute of Science

    [\[page\]](https://diffusion-tokenflow.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2307.10373) 
    [\[code coming\]](https://github.com/omerbt/TokenFlow)

- **VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet**

    (2023.7.26) <code>arXiv</code> Beihang University, University of Hong Kong 

    [\[page\]](https://vcg-aigc.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2307.14073) 
    [\[code coming\]](https://github.com/ZhihaoHu/VideoControlNet)
<!-- 
    (2023..) <code></code>  None

    [\[page\]]() 
    [\[paper\]]() 
    [\[code\]]()
-->
    
