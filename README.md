## paper list

### base model

- **[VDM] Video Diffusion Models**

    (2022.04.07) NIPS 2022 
    [\[page\]](https://video-diffusion.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2204.03458) 
    [\[code\]](https://github.com/lucidrains/video-diffusion-pytorch)
    Google

- **MCVD：Masked conditional video diffusion for prediction, generation, and interpolation**

    (2022.05.19) NIPS2022
    [\[page\]](https://mask-cond-video-diffusion.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2205.09853) 
    [\[code\]](https://github.com/voletiv/mcvd-pytorch)

- **[FDM] Flexible Diffusion Modeling of Long Videos**

    (2022.05.23) arXiv 
    [\[page\]](https://www.cs.ubc.ca/~wsgh/fdm/) 
    [\[paper\]](https://arxiv.org/abs/2205.11495) 
    [\[code\]](https://github.com/plai-group/flexible-video-diffusion-modeling)
    
- **CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers Paper**

    (2022.05.29) ICLR 2023 
    [\[page\]](https://models.aminer.cn/cogvideo/) 
    [\[paper\]](https://arxiv.org/abs/2205.15868) 
    [\[code\]](https://github.com/THUDM/CogVideo)
    THU

- **Make-A-Video：Text-to-Video Generation without Text-Video Data**
  
    (2022.09.29) ICLR 2023
    [\[page\]](https://makeavideo.studio/) 
    [\[paper\]](https://arxiv.org/abs/2209.14792) 
    [\[code\]](https://github.com/lucidrains/make-a-video-pytorch)
    Meta

- **IMAGEN VIDEO: HIGH DEFINITION VIDEO GENERATION WITH DIFFUSION MODELS**
  
    (2022.10.05) arXiv
    [\[page\]](https://imagen.research.google/video/) 
    [\[paper\]](https://arxiv.org/abs/2210.02303) 
    [code]
    Google
  
- **MagicVideo: Efficient Video Generation With Latent Diffusion Models**

    (2022.11.20) arXiv
    [\[page\]](https://magicvideo.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2211.11018) 
    [code]
    ByteDance

- **LVDM：Latent Video Diffusion Models for High-Fidelity Long Video Generation**

    (2022.11.23) arXiv
    [\[page\]](https://yingqinghe.github.io/LVDM/) 
    [\[paper\]](https://arxiv.org/abs/2211.13221) 
    [\[code\]](https://github.com/YingqingHe/LVDM)
    HKUST Tencent

- **[PVDM] Video Probabilistic Diffusion Models in Projected Latent Space**
  
    (2023.02.15) CVPR 2023
    [\[page\]](https://sihyun.me/PVDM/) 
    [\[paper\]](https://arxiv.org/abs/2302.07685) 
    [\[code\]](https://github.com/sihyun-yu/PVDM)
    Google

- **VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation**

    (2023.3.15) CVPR2023
    [\[page\]](https://www.modelscope.cn/models/damo/text-to-video-synthesis/summary) 
    [\[paper\]](https://arxiv.org/abs/2303.08320) 
    [\[code\]](https://www.modelscope.cn/models/damo/text-to-video-synthesis/files)
    Alibaba UCAS

### video editing

- **Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation**

    (2022.12.22) arXiv
    [\[page\]](https://tuneavideo.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2212.11565) 
    [\[code\]](https://github.com/showlab/Tune-A-Video)
    Tencent(PCG) NUS

- **Gen-1: Structure and Content-Guided Video Synthesis with Diffusion Models**

    (2023.2.6) arXiv
    [\[page\]](https://research.runwayml.com/gen1) 
    [\[paper\]](https://arxiv.org/abs/2302.03011) 
    [code]
    Runway

- **Video-P2P: Video Editing with Cross-attention Control**

    (2023.3.8) arXiv
    [\[page\]](https://video-p2p.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2303.04761) 
    [\[code\]](https://github.com/ShaoTengLiu/Video-P2P)
    Adobe CUHK
    
- **FateZero: Fusing Attentions for Zero-shot Text-based Video Editing**

    (2023.3.16) arXiv
    [\[page\]](https://fate-zero-edit.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2303.09535) 
    [\[code\]](https://github.com/ChenyangQiQi/FateZero)
    Tencent(AI Lab) HKUST

- **Pix2video: Video editing using image diffusion**

    (2023.3.22) arXiv
    [\[page\]](https://duyguceylan.github.io/pix2video.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2303.12688) 
    [\[code\]](https://github.com/G-U-N/Pix2Video.pytorch)
    Adobe

- **Text2video-zero: Text-toimage diffusion models are zero-shot video generators.**

    (2023.3.23) arXiv
    [\[page\]](https://text2video-zero.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2303.13439) 
    [\[code\]](https://github.com/Picsart-AI-Research/Text2Video-Zero)
    Picsart

- **vid2vid-zero: Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models**

    (2023.3.30) arXiv
    [\[page\]](https://github.com/baaivision/vid2vid-zero#examples) 
    [\[paper\]](https://arxiv.org/abs/2303.17599) 
    [\[code\]](https://github.com/baaivision/vid2vid-zero)
    BAAI ZJU

- **ControlVideo: Training-free Controllable Text-to-Video Generation**

    (2023.5.22) arXiv
    [\[page\]](https://github.com/YBYBZhang/ControlVideo#visualizations) 
    [\[paper\]](https://arxiv.org/abs/2305.13077) 
    [\[code\]](https://github.com/YBYBZhang/ControlVideo)
    Huawei HIT

- **ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing**

    (2023.5.26) arXiv
    [\[page\]](https://ml.cs.tsinghua.edu.cn/controlvideo/) 
    [\[paper\]](https://arxiv.org/abs/2305.17098) 
    [\[code\]](https://github.com/thu-ml/controlvideo)
    Tsinghua 
    
- **[EI^2] Towards Consistent Video Editing with Text-to-Image Diffusion Models**

    (2023.5.27) arXiv
    [page]
    [\[paper\]](https://arxiv.org/abs/2305.17431) 
    [code]
    Meitu, UCAS
    
- **Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising**
  
    (2023.5.29) arXiv
    [\[page\]](https://g-u-n.github.io/projects/gen-long-video/index.html) 
    [\[paper\]](https://arxiv.org/abs/2305.18264) 
    [\[code\]](https://github.com/G-U-N/Gen-L-Video)
    Shanghai AI Lab CUHK(mmLab)

- **Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation**

    (2023.6.13) arXiv
    [\[page\]](https://anonymous-31415926.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2306.07954) 
    [code]
    NTU

- **VidEdit: Zero-shot and Spatially Aware Text-driven Video Editing**

    (2023.6.14) arXiv
    [\[page\]](https://videdit.github.io/) 
    [\[paper\]](https://arxiv.org/abs/2306.08707) 
    [code]
    Paris, France

<!-- 
    (2023..) None
    [\[page\]]() 
    [\[paper\]]() 
    [\[code\]]()
    None
-->
